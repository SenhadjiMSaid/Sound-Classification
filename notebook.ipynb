{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNutt5QE23XGYtCSwgEl9Du",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SenhadjiMSaid/Sound-Classification/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efficient Deep Learning Approach for Multiclass Sound Classification\n",
        "\n",
        "> This a semester final mini project of deep learning 2CSI\n",
        "\n",
        "**Authors**:\n"
      ],
      "metadata": {
        "id": "PSnCxkg9Tc3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Dependencies"
      ],
      "metadata": {
        "id": "1drloOtPUwlO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t1gmBZFnTVi6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio # for audio processing\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB, TimeMasking, FrequencyMasking\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "from tqdm import tqdm\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qKKFo3TttbT",
        "outputId": "57e10da7-e9fc-4e1f-f35a-a510082739f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration setup\n",
        "\n",
        "These are the key **hyperparameters** for training:\n",
        "\n",
        "- `sample_rate`: The number of samples per second in an audio clip.\n",
        "- `n_mels`: Number of mel-frequency bins in spectrograms.\n",
        "- `batch_size`: Number of samples per batch for training.\n",
        "- `epochs`: Total training cycles.\n",
        "- `learning_rate`: Step size for optimizer updates.\n",
        "- `train_split`: Proportion of data used for training.\n",
        "- `patience`: How long to wait before early stopping."
      ],
      "metadata": {
        "id": "m9CgFkFtU3ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    \"sample_rate\": 16000,\n",
        "    \"n_mels\": 128,\n",
        "    \"n_fft\": 1024,\n",
        "    \"hop_length\": 512,\n",
        "    \"batch_size\": 32,\n",
        "    \"epochs\": 60,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"audio_duration\": 2,\n",
        "    \"num_classes\": 10,\n",
        "    \"train_split\": 0.8,\n",
        "    \"patience\": 10,\n",
        "    \"lr_decay_factor\": 0.5,\n",
        "    \"lr_decay_patience\": 2,\n",
        "    \"weight_decay\": 5e-4,\n",
        "    \"mixup_alpha\": 0.4  # Mixup hyperparameter (higher means stronger mixup)\n",
        "}"
      ],
      "metadata": {
        "id": "WDuDmiMAVOMZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the AudioDataset class"
      ],
      "metadata": {
        "id": "dOrfws8mVSdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, train=True):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.train = train\n",
        "\n",
        "        self.mel_spec_transform = MelSpectrogram(\n",
        "            sample_rate=CONFIG[\"sample_rate\"],\n",
        "            n_fft=CONFIG[\"n_fft\"],\n",
        "            hop_length=CONFIG[\"hop_length\"],\n",
        "            n_mels=CONFIG[\"n_mels\"]\n",
        "        )\n",
        "        self.amp_to_db = AmplitudeToDB(top_db=80)\n",
        "\n",
        "        # Spectrogram augmentations (SpecAugment)\n",
        "        self.time_mask = TimeMasking(time_mask_param=20)\n",
        "        self.freq_mask = FrequencyMasking(freq_mask_param=10)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        waveform, sr = torchaudio.load(audio_path)\n",
        "        if sr != CONFIG[\"sample_rate\"]:\n",
        "            waveform = torchaudio.transforms.Resample(sr, CONFIG[\"sample_rate\"])(waveform)\n",
        "\n",
        "        # Ensure consistent length\n",
        "        target_length = int(CONFIG[\"audio_duration\"] * CONFIG[\"sample_rate\"])\n",
        "        if waveform.size(1) > target_length:\n",
        "            start = np.random.randint(0, waveform.size(1) - target_length) if self.train else 0\n",
        "            waveform = waveform[:, start:start + target_length]\n",
        "        else:\n",
        "            waveform = F.pad(waveform, (0, target_length - waveform.size(1)))\n",
        "\n",
        "        # ----- Waveform Augmentations -----\n",
        "        if self.train:\n",
        "            waveform = torch.roll(waveform, shifts=np.random.randint(-1600, 1600), dims=1)\n",
        "            waveform = waveform * np.random.uniform(0.8, 1.2)\n",
        "        # -----------------------------------\n",
        "\n",
        "        mel_spec = self.mel_spec_transform(waveform)\n",
        "        mel_spec = self.amp_to_db(mel_spec)\n",
        "        mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-8)  # Normalize\n",
        "\n",
        "        if self.train:\n",
        "            mel_spec = self.time_mask(mel_spec)\n",
        "            mel_spec = self.freq_mask(mel_spec)\n",
        "\n",
        "        return mel_spec, label\n"
      ],
      "metadata": {
        "id": "iqCIU7rqVZEd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mixup** is a data augmentation technique where two training samples are mixed together using a random weight $Î»$.\n",
        "\n",
        "- It helps improve model generalization.\n",
        "- It prevents the model from becoming overconfident.\n",
        "- Instead of training on a single label, the model learns a weighted combination of labels."
      ],
      "metadata": {
        "id": "th4BXQhOVYU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=CONFIG[\"mixup_alpha\"]):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ],
      "metadata": {
        "id": "KpvZz6l5VpD2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to display a spectrogram"
      ],
      "metadata": {
        "id": "DTA-kZQLVx8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spectrogram(file_path):\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    mel_spec_transform = MelSpectrogram(\n",
        "        sample_rate=CONFIG[\"sample_rate\"],\n",
        "        n_fft=CONFIG[\"n_fft\"],\n",
        "        hop_length=CONFIG[\"hop_length\"],\n",
        "        n_mels=CONFIG[\"n_mels\"]\n",
        "    )\n",
        "    mel_spec = mel_spec_transform(waveform)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.imshow(mel_spec.log2()[0].numpy(), cmap=\"inferno\", aspect=\"auto\")\n",
        "    plt.title(\"Mel Spectrogram\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tTuxyv7DV0wt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the model architecture"
      ],
      "metadata": {
        "id": "MQLrjKk4V98c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientResNetAudio(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=1):\n",
        "        super(EfficientResNetAudio, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=False)\n",
        "        self.resnet.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        in_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)"
      ],
      "metadata": {
        "id": "nDiNZ9LJV-9V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining how the training epoch should be like"
      ],
      "metadata": {
        "id": "02cR7IwEWdWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss, all_preds, all_labels = 0, [], []\n",
        "\n",
        "    for data, target in tqdm(train_loader, desc=\"Training\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Apply Mixup augmentation\n",
        "        mixed_data, targets_a, targets_b, lam = mixup_data(data, target, CONFIG[\"mixup_alpha\"])\n",
        "        output = model(mixed_data)\n",
        "\n",
        "        loss = mixup_criterion(criterion, output, targets_a, targets_b, lam)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        all_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
        "        all_labels.extend(target.cpu().numpy())\n",
        "\n",
        "    return total_loss / len(train_loader), f1_score(all_labels, all_preds, average='macro')"
      ],
      "metadata": {
        "id": "BUO9GIadWeZu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "tjZQAa6RWjgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzERNRCjWhCg",
        "outputId": "f3e62a98-5e69-4dae-cb46-ef75d94e7e1e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/mini-projet-DL/train\""
      ],
      "metadata": {
        "id": "OnPjq6vdtkvT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths, labels = [], []\n",
        "\n",
        "for label_idx, class_name in enumerate(sorted(os.listdir(data_dir))):\n",
        "    class_dir = os.path.join(data_dir, class_name)\n",
        "    for file_name in os.listdir(class_dir):\n",
        "        file_paths.append(os.path.join(class_dir, file_name))\n",
        "        labels.append(label_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "sFNXSB2DWmeW",
        "outputId": "ffe7586b-fc06-45ae-d796-bc62dd583e94"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "[Errno 20] Not a directory: '/content/drive/MyDrive/mini-projet-DL/train/airport.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-91f437a4e276>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclass_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mfile_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/drive/MyDrive/mini-projet-DL/train/airport.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_spectrogram(data_dir)"
      ],
      "metadata": {
        "id": "Sb1VnBsiyuoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = AudioDataset(file_paths, labels, train=True)\n",
        "train_size = int(CONFIG[\"train_split\"] * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ],
      "metadata": {
        "id": "GpF0Y12uWr9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "A-G3zxS-Wxid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EfficientResNetAudio(num_classes=CONFIG[\"num_classes\"]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])"
      ],
      "metadata": {
        "id": "Iojn2cZzW0_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(CONFIG[\"epochs\"]):\n",
        "    train_epoch(model, train_loader, criterion, optimizer, device)"
      ],
      "metadata": {
        "id": "Vd9E9eoWW37V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'best_model.pth')"
      ],
      "metadata": {
        "id": "yX_pv3Hjyibn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "NqnhLgHUW7J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "6uKr06idW809"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device, class_names=None):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of the model with detailed metrics and visualizations\n",
        "\n",
        "    Args:\n",
        "        model: The trained model to evaluate\n",
        "        test_loader: DataLoader for test data\n",
        "        device: Device to use for computation\n",
        "        class_names: List of class names (optional)\n",
        "    \"\"\"\n",
        "    if class_names is None:\n",
        "        class_names = [\n",
        "            \"airport\", \"bus\", \"metro\", \"metro_station\",\n",
        "            \"park\", \"public_square\", \"shopping_mall\",\n",
        "            \"street_pedestrian\", \"street_traffic\", \"tram\"\n",
        "        ]\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    # Collect predictions\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            preds = output.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(target.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    # Print overall metrics\n",
        "    print(f\"Test F1 Score (Macro): {f1:.4f}\")\n",
        "    print(f\"Precision (Macro): {precision:.4f}\")\n",
        "    print(f\"Recall (Macro): {recall:.4f}\")\n",
        "\n",
        "    # Generate and plot confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names\n",
        "    )\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig('confusion_matrix.png', bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Generate per-class metrics\n",
        "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    # Save metrics to CSV\n",
        "    report_df.to_csv('classification_report.csv')\n",
        "    print(\"Classification report saved to 'classification_report.csv'\")\n",
        "\n",
        "    # Show per-class F1 scores\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    class_f1_scores = [report[c]['f1-score'] for c in class_names]\n",
        "    bars = plt.bar(class_names, class_f1_scores)\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.title('F1 Score per Class')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                 f'{height:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('f1_scores_per_class.png', bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Save all results to a single file\n",
        "    results = {\n",
        "        'f1_macro': f1,\n",
        "        'precision_macro': precision,\n",
        "        'recall_macro': recall,\n",
        "        'per_class_metrics': report\n",
        "    }\n",
        "\n",
        "    # Save as pickle for later use\n",
        "    import pickle\n",
        "    with open('evaluation_results.pkl', 'wb') as f:\n",
        "        pickle.dump(results, f)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "9WHEobINXBoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = evaluate_model(model, val_loader, device, class_names)"
      ],
      "metadata": {
        "id": "8IJpOVMnXHbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}